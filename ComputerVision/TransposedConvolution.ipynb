{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转置卷积\n",
    "\n",
    "卷积层通常会使图像的长宽减少，如果有Poling层的话，那么会更加地减少，如果我们的输出图像的尺度并且输入图像相同(这些任务很常见，图像翻译等任务)。\n",
    "\n",
    "为了实现以上的目标，采用转置卷积ConvTranspose2d来进行上采样，这是2016年提出的，用于逆转下采样造成的空间尺寸的减小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pltutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设步幅为1且没有填充。\n",
    "\n",
    "假设有 $n_h \\times n_w$ 的输入张量和一个 $k_h \\times k_w$ 的卷积核，以步幅为1滑动卷积核窗口，每行 $n_w$ 次，每列  $n_h$ 次，共产生了 $n_h n_w$ 个中间结果。\n",
    "\n",
    "每个中间结果都是一个 $(n_h+k_h -1) \\times (n_w + k_w -1)$ 的张量，初始化为0。为了计算每个中间张量，输入张量中的每个元素都要乘以卷积核，从而使 $k_h \\times k_w$ 张量替换中间张量的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们对输入矩阵X和卷积核矩阵K实现基本的转置卷积运算trans_conv。\n",
    "def trans_conv(X:t.Tensor,K:t.Tensor)->t.Tensor:\n",
    "    h,w = K.shape\n",
    "    Y = t.zeros((X.shape[0]+h-1,X.shape[1]+w-1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            # 注意这里是+=\n",
    "            Y[i:i+h,j:j+w] += X[i,j]*K\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.],\n",
       "        [ 0.,  4.,  6.],\n",
       "        [ 4., 12.,  9.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =t.tensor([[0.,1.],[2.,3.]])\n",
    "K =t.tensor([[0.,1.],[2.,3.]])\n",
    "trans_conv(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 0.,  4.,  6.],\n",
       "          [ 4., 12.,  9.]]]], grad_fn=<SlowConvTranspose2DBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当输入X和卷积核K都是四维的张量的时候，我们可以使用高级API来获得相同的效果。\n",
    "X,K = X.reshape(1,1,2,2,),K.reshape(1,1,2,2)\n",
    "tconv= nn.ConvTranspose2d(1,1,kernel_size=2,bias=False)\n",
    "tconv.weight.data=K\n",
    "tconv.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding, Stride and MultiChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.]]]], grad_fn=<SlowConvTranspose2DBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "将高和宽两侧的填充数指定为1的时候，转置卷积的输出中将删除第一和最后的行和列。\n",
    "\"\"\"\n",
    "tconv = nn.ConvTranspose2d(1,1,kernel_size=2,padding=1,bias=False)\n",
    "tconv.weight.data=K\n",
    "tconv.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 1.],\n",
       "          [0., 0., 2., 3.],\n",
       "          [0., 2., 0., 3.],\n",
       "          [4., 6., 6., 9.]]]], grad_fn=<SlowConvTranspose2DBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "步幅为2的时候会增加长宽\n",
    "\"\"\"\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与矩阵变换的关系\n",
    "\n",
    "给定输入向量$x$和权重矩阵$W$，卷积的前向传播函数可以通过将其输入与权重矩阵相乘并输出向量$y = Wx$实现，反向传播遵循链式法则$ grad \\ xy =W^T$\n",
    "\n",
    "使用矩阵应该比上面的for循环要快"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b663eacfcde8320daa8b7b495561284796d4e9f74e2ac923461b7db3a12d2a31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
