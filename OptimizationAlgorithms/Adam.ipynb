{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adam 算法\n",
    "**集大成者**\n",
    "\n",
    "Adam虽然是DL中使用的更强大和有效的优化算法之一，它也有一些缺点：Adam算法可能由于方差控制不良而发散，18年的时候有人给Adam算法提供了一个称为Yogi的热补丁，解决了以上的问题。\n",
    "\n",
    "## 具体公式\n",
    "Adam速算法使用指数加权移动平均值来估算梯度的动量和二次矩，即使用状态变量：\n",
    "$$\n",
    "v_t \\gets \\beta_1 v_{t-1} + (1-\\beta_1)g_t,\\\\\n",
    "s_t \\gets \\beta_2 s_{t-1} + (1-\\beta_2) g_t^2\n",
    "$$\n",
    "一般取$$\\beta_1 = 0.9, \\beta_2 = 0.999$$，方差估计的移动远远比动量估计移动的慢。同样地，我们需要使用:\n",
    "$$\n",
    "\\sum_{t=0}^t \\beta^i = \\frac{1-\\beta^t}{1- \\beta}\n",
    "$$\n",
    "来对$v_t,s_t$来进行归一化：\n",
    "$$\n",
    "\\hat v_t = \\frac {v_t}{1-\\beta_1^t}\\\\\n",
    "\\hat s_t = \\frac {s_t}{1-\\beta_2^t}\n",
    "$$\n",
    "\n",
    "有了正确的估计$\\hat v_t ,\\hat s_t$，可以使用非常类似于RMSProp算法的方式重新缩放梯度：\n",
    "$$\n",
    "g_t^\\prime = \\frac{ \\eta \\hat v_t}{\\sqrt{\\hat s_t}+ \\epsilon}\n",
    "$$\n",
    "\n",
    "与RMSProp不同的是：使用$\\hat v_t$来更新梯度而不是梯度$g_t$本身，Adam使用${\\sqrt{\\hat s_t}+ \\epsilon}$而不是${\\sqrt{\\hat s_t+ \\epsilon}}$进行缩放。\n",
    "\n",
    "最后的最后，使用平滑之后的梯度对参数进行更新：\n",
    "$$x_t \\gets x_{t-1} - g_t^\\prime$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def init_state(feature_dim):\n",
    "    v_w,v_b = t.zeros((feature_dim,1)),t.zeros(1)\n",
    "    s_w,s_b = t.zeros((feature_dim,1)),t.zeros(1)\n",
    "    return((v_w,s_w),(v_b,s_b))\n",
    "\n",
    "def adam(params:t.Tensor,states,hyperparams):\n",
    "    beta1 ,beta2 ,eps = 0.9,0.999,1e-6\n",
    "    for p,(v,s) in zip(params,states):\n",
    "        with t.no_grad():\n",
    "            v[:] = beta1 * v + (1-beta1) * p.grad\n",
    "            s[:] = beta2 * s + (1-beta2) * t.square(p.grad)\n",
    "            v_bias_corr = v/(1-beta1 ** hyperparams[\"t\"])\n",
    "            s_bias_corr = s/(1-beta2**hyperparams[\"t\"])\n",
    "            p[:] -= hyperparams[\"lr\"] * v_bias_corr /(t.sqrt(s_bias_corr)+eps)\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "    hyperparams[\"t\"] += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
