{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "\n",
    "Adadelta是AdaGrad的又一种变体，前者减少了学习率适应坐标的数量。\n",
    "\n",
    "广义上Adadelta没有学习率，它使用变化量本身作为未来变化的校准，它是在2012年被提出的\n",
    "\n",
    "使用两个状态变量$s_t$存储梯度二阶导数的泄露平均值，$\\Delta \\mathbf{x}$来存储模型本身中参数变化二阶导数的泄露平均值。\n",
    "下面的命名都是作者文章中的原始符号，其实没有必要替换掉与AdaGrad等方法中的用途一致的参数。\n",
    "\n",
    "## 细节\n",
    "使用RMSProp类似的泄露更新：\n",
    "$$\n",
    "s_t = \\rho s_{t-1} + (1-\\rho)g_t^2\n",
    "$$\n",
    "\n",
    "然后调整梯度：\n",
    "\n",
    "$$\n",
    "g_t^\\prime = \\frac {\\sqrt{\\Delta x_{t-1} + \\epsilon}}{\\sqrt{s_t + \\epsilon}} \\odot g_t\n",
    "$$\n",
    "\n",
    "其中$ \\Delta x_{t-1}$是重新缩放梯度$g_t^\\prime$的泄露平均值，将$ \\Delta x_0$初始化为0，然后在每个步骤中使用$g_t^\\prime$去更新它，即：\n",
    "$$\n",
    "\\Delta x_t = \\rho \\Delta x_{t-1} + (1-\\rho){g_t^\\prime}^2\n",
    "$$\n",
    "\n",
    "同样地，在分母分子上面加上epsilon是为了增加数值稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码实现\n",
    "import torch as t\n",
    "\n",
    "def init_adadelta_states(features_dim):\n",
    "    s_w,s_b = t.zeros((features_dim,1)),t.zeros(1)\n",
    "    delta_w,delta_b = t.zeros((features_dim,1)),t.zeros(1)\n",
    "    return ((s_w,s_b),(delta_w,delta_b))\n",
    "\n",
    "def adadelta(params,states,hyperparams):\n",
    "    rho,eps = hyperparams[\"rho\"],1e-5\n",
    "    for p,(s,delta) in zip (params,states):\n",
    "        with t.no_grad():\n",
    "            s[:] = rho * s + (1-rho)*t.square(p.grad)\n",
    "            g = (t.sqrt(delta+eps)/t.sqrt(s+eps))*p.grad\n",
    "            p[:] -=g\n",
    "            delta[:]=rho*delta +(1-rho)*g*g\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
