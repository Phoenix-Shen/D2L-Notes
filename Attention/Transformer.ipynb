{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "自注意力与CNN和RNN相比具有并行计算和最短的最大路径长度这两个优势，所以使用自注意力来设计深度架构是很有吸引力的.\n",
    "\n",
    "Transformer基于Encoder-Decoder架构来处理序列对，跟使用注意力的Seq2Seq不同，它是基于纯注意力的，没有RNN的参与。\n",
    "\n",
    "## Component\n",
    "1. 多头注意力\n",
    "2. 有掩码的多头注意力\n",
    "3. 基于位置的前馈网络\n",
    "4. 层归一化 Layer Norm，Batch Norm对特征维度进行归一化，但是对于不定长数据来说不太行。\n",
    "5. Message Passing\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "- transformer 是一个纯使用注意力的编码-解码器\n",
    "- 编码器和解码器都有n个transformer块\n",
    "- 每个块里头使用多头自注意力，基于位置的前馈网络和层归一化函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pltutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义基于位置的前馈网络\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_outputs,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_outputs)\n",
    "    \n",
    "    def forward(self,X:t.Tensor):\n",
    "        return self.dense2.forward(\n",
    "            relu(\n",
    "                self.dense1.forward(X)\n",
    "            )\n",
    "        )\n",
    "\n",
    "# 测试一下\n",
    "ffn = PositionWiseFFN(4,4,8)\n",
    "ffn(t.ones((2,3,4))).shape\n",
    "# 线性层会将最后一维除外的维度都看做batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000,  1.0000],\n",
       "         [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "ln=nn.LayerNorm(2)\n",
    "bn=nn.BatchNorm1d(2)\n",
    "X=t.tensor([[1,2,],[2,3]],dtype=t.float32)\n",
    "ln(X),bn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差连接\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y)+X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们有了组成编码器的基本组件，先来实现编码器中的一个层\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer 编码器块\n",
    "    \"\"\"\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,use_bias=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            key_size,query_size,value_size,num_hiddens,num_heads,dropout,use_bias\n",
    "        )\n",
    "        self.addnorm1 = AddNorm(norm_shape,dropout,)\n",
    "        self.ffn =PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape,dropout,)\n",
    "    \n",
    "    def forward (self,X:t.Tensor,valid_lens:t.Tensor):\n",
    "        Y =self.addnorm1(X,self.attention(X,X,X,valid_lens))\n",
    "        return self.addnorm2(Y,self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformer编码器中的任何层都不会改变其输入的形状\n",
    "X = t.ones((2,100,24))\n",
    "valid_lens =t.tensor([3,2])\n",
    "encoder_blk = EncoderBlock(\n",
    "    24,24,24,24,[100,24],24,48,8,0.5\n",
    ")\n",
    "encoder_blk.eval()\n",
    "encoder_blk(X, valid_lens).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在实现num_layers个EncoderBlock的编码器\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,key_size,query_size,value_size,num_hiddens,\n",
    "    norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,\n",
    "    use_bias=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_endoding = PositionalEncoding(num_hiddens,dropout)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "            EncoderBlock(\n",
    "                key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,use_bias)\n",
    "            )\n",
    "        \n",
    "    def forward(self,X:t.Tensor,valid_lens:t.Tensor,*args):\n",
    "        X =self.pos_endoding(self.embedding.forward(X)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None]*len(self.blks)\n",
    "\n",
    "        for i ,blk in enumerate(self.blks):\n",
    "            X =blk(X,valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器也是由多个相同的层组成，在DecoderBlock类中实现的每个层都包含了三个子层：解码器自注意力、\n",
    "# 编码器-解码器注意力和基于位置的前馈网络\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,\n",
    "    norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.i=i\n",
    "        self.attention1 =MultiHeadAttention(\n",
    "            key_size,query_size,value_size,num_hiddens,num_heads,dropout\n",
    "        )\n",
    "        self.addnorm1 =AddNorm(norm_shape,dropout)\n",
    "        self.attention2 =MultiHeadAttention(\n",
    "            key_size,query_size,value_size,num_hiddens,num_heads,dropout\n",
    "        )\n",
    "        self.addnorm2 =AddNorm(norm_shape,dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens)\n",
    "        self.addnorm3 =AddNorm(norm_shape,dropout)\n",
    "    \n",
    "    def foreward(self, X: t.Tensor, state: tuple[t.Tensor, t.Tensor, t.Tensor]):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values =X \n",
    "        else:\n",
    "            key_values = t.cat((state[2][self.i],X),dim=1)\n",
    "        state[2][self.i]=key_values\n",
    "        if self.training:\n",
    "            batch_size,num_steps,_ = X.shape\n",
    "            dec_valid_lens = t.arange(1,num_steps+1,device=X.device).repeat(batch_size,1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        X2 = self.attention1(X,key_values,key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        Y2=self.attention2(Y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        Z =self.addnorm2(Y,Y2)\n",
    "        return self.addnorm3(Z,self.ffn(Z)),state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                                 DecoderBlock(key_size, query_size, value_size, num_hiddens,\n",
    "                                              norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                                              num_heads, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # 解码器自注意力权重\n",
    "            self._attention_weights[0][\n",
    "                i] = blk.attention1.attention.attention_weights\n",
    "            # “编码器－解码器”自注意力权重\n",
    "            self._attention_weights[1][\n",
    "                i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管transformer架构是为了seq2seq的学习而提出的，但是transformer编码器或者是它的解码器通常被单独用于不同的学习任务中。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
