{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力\n",
    "\n",
    "在实践中，给定相同的查询、键、值的组合的时候，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列中各种范围的依赖关系，因此允许注意力机制组合使用查询、键和值的不同子空间表示可能是有益的。\n",
    "\n",
    "对此，我们可以学习得到$h$组不同的线性投影来变换查询、键和值，然后这$h$组变换后的查询、键和值将并行地送入注意力汇聚中。最后这$h$个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影变换，最终产生输出，这种设计被称为多头注意力，这是在17年被提出的，对于$h$个注意力汇聚输出，每一个注意力汇聚都被称为一个头(head)\n",
    "\n",
    "## 模型\n",
    "在实现多头注意力之前，我们使用数学语言将这个模型形式化地描述出来。\n",
    "给定查询键、值分别为:\n",
    "$$\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\mathbf{q} &\\in \\mathbb{R}^{d_q}\\\\\n",
    "\\mathbf{k} &\\in \\mathbb{R}^{d_k}\\\\\n",
    "\\mathbf{v} &\\in \\mathbb{R}^{d_v}\\\\\n",
    "\\end{aligned}\\end{split}\n",
    "$$\n",
    "每个注意力头$ \\mathbf{h}_i (i=1,\\dots,h)$的计算方法为：\n",
    "$$\n",
    "\\mathbf{h}_i = f(\\mathbf{W}_i^{(q)}\\mathbf{q},\\mathbf{W}_i^{(k)}\\mathbf{k},\\mathbf{W}_i^{(v)}\\mathbf{v}) \\in \\mathbb{R}^{p_v}\n",
    "$$\n",
    "其中可学习的参数有：\n",
    "$$\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\mathbf{W}_i^{(q)} &\\in \\mathbb{R}^{p_q \\times d_q}\\\\\n",
    "\\mathbf{W}_i^{(k)} &\\in \\mathbb{R}^{p_k \\times d_k}\\\\\n",
    "\\mathbf{W}_i^{(v)} &\\in \\mathbb{R}^{p_v \\times d_v}\\\\\n",
    "\\end{aligned}\\end{split}\n",
    "$$\n",
    "\n",
    "此外，多头注意力的输出还需要经过线性变换，所以它也有可以学习的参数\n",
    "$$ \\mathbf{W}_o \\in \\mathbb{R}^{p_o \\times hp_v} $$\n",
    "$$ \\begin{split}\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.\\end{split}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from pltutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现过程中，我们使用缩放点积注意力作为每一个注意力头，为了避免计算代价，设置$ p_q=p_k=p_b = p_o /h_o$ 这样就可以实现并行计算，下面的实现中$p_o$是通过num_hiddens实现的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X:torch.Tensor,num_heads:int):\n",
    "    \"\"\"\n",
    "    为了多注意力的并行计算而转换形状\n",
    "    \"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)\n",
    "    X=X.reshape(X.shape[0],X.shape[1],num_heads,-1)\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # 最终输出 (batch_size*num_heads,查询或者“键－值”对的个数,num_hiddens/num_heads)\n",
    "    return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "\n",
    "\n",
    "def transpose_output(X:torch.Tensor, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention=DotProductAttention(dropout)\n",
    "        self.W_q =nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o =nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "    \n",
    "\n",
    "    def forward(self,queries,keys,values,valid_lens):\n",
    "        # 将这些玩意拆成batch实现并行化\n",
    "        # queries, keys, values.shape = (batch_size, num_of_q/k/v s,num_hiddens)\n",
    "        # valied_lens.shape = (batch_size,num_queries)\n",
    "        queries=transpose_qkv(self.W_q.forward(queries),self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k.forward(keys),self.num_heads)\n",
    "        values = transpose_qkv(self.W_v.forward(values),self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = t.repeat_interleave(valid_lens,repeats=self.num_heads,dim=0)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "num_hiddens, num_heads = 128, 8\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()\n",
    "\n",
    "\n",
    "# 输入实际数据进行测试\n",
    "querey= t.zeros((2,1,128))\n",
    "keys =t.zeros((2,5,128))\n",
    "values = t.ones((2,5,128))\n",
    "\n",
    "output = attention.forward(querey,keys,values,None)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
