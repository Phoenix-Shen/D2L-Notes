{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "\n",
    "单词$x_t$在时间步$t$的条件概率仅仅取决于前面的$n-1$个单词，而对于之前$t-(n-1)$的单词，要想将影响合并到$x_t$上，需要增加$n$，模型的参数量会成指数性增长，因为词表$\\mathcal {V}$需要存储$|\\mathcal {V}|^n$个数字，因此使用隐变量模型\n",
    "$$P(x_t \\mid x_{t-1},\\dots,x_1) \\approx P(x_t \\mid h_{t-1})$$\n",
    "\n",
    "其中$h_{t-1}$是隐状态，也成为了隐藏变量，它存储了到时间步$t-1$的序列信息，通常，我们可以基于当前的输入$x_t,h_{t-1}$来计算时间步t处的任何时间的隐状态：\n",
    "$$h_t = f(x_t,h_{t-1})$$\n",
    "\n",
    "*RNN是具有隐状态的神经网络*\n",
    "## 无隐状态的神经网络\n",
    "给定小批量样本$ \\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，则隐藏层的输出$ \\mathbf{H} \\in \\mathbb{R}^{n \\times h}$通过下式计算：\n",
    "$$\\mathbf{H}= \\phi(\\mathbf{H}\\mathbf{W}_{xh}+\\mathbf {b}_h )$$\n",
    "\n",
    "之后我们有隐藏层的参数$\\mathbf{W} \\in \\mathbb{R}^{d \\times h}$，偏置参数为$b_h \\in \\mathbb{R}^{1 \\times h}$，以$\\mathbf{H}$作为输入，输入为：\n",
    "$$\\mathbf{O}= \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n",
    "\n",
    "## 有隐状态的循环神经网络\n",
    "\n",
    "假设在时间步$t$有小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$。对于$n$个序列样本的小批量，$\\mathbf{X}_t$的每一行对应来自于该序列的时间步$t$处的一个样本，截下来用$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$表示时间步t的隐藏变量，与MLP不同的是，在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_{t-1}$,并且引入新的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$\n",
    "\n",
    "当前步的隐藏变量，由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：\n",
    "$$\\mathbf{H}_t = \\phi (\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}+\\mathbf{b}_h)$$\n",
    "\n",
    "对于输出有:\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n",
    "\n",
    "由于在当前的时间步$t$中，隐状态的定义与前一个时间步中使用的定义相同，因此上面的公式计算是`循环的`(Recurrent)，基于循环计算的隐状态神经网络被称为`循环神经网络`(recurrent neural networks, RNNs),在RNN中，执行循环计算的层被称为`循环层`(recurren layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3878,  2.8297,  1.5097, -4.3425],\n",
       "        [-0.6364, -0.9101, -3.7470,  0.3966],\n",
       "        [-0.6947,  0.2756, -0.2900, -1.9641]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "X,W_xh=t.normal(0,1,(3,1)),t.normal(0,1,(1,4))\n",
    "H,W_hh=t.normal(0,1,(3,4)),t.normal(0,1,(4,4))\n",
    "t.matmul(X,W_xh)+t.matmul(H,W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3878,  2.8297,  1.5097, -4.3425],\n",
       "        [-0.6364, -0.9101, -3.7470,  0.3966],\n",
       "        [-0.6947,  0.2756, -0.2900, -1.9641]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.matmul(t.cat((X,H),1),t.cat((W_xh,W_hh),0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
